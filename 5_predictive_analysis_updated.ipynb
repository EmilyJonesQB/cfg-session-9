{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import statsmodels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A classification problem : the Adult Income dataset\n",
    "\n",
    "Another common use case in Supervised Learning is classification. Let's try one example now with the **Adult Income** dataset. This dataset was extracted from the 1994 Census database and is described by the following variables:\n",
    "\n",
    "- **age**: continuous.\n",
    "- **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "- **fnlwgt**: continuous.\n",
    "- **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "- **education-num**: continuous.\n",
    "- **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "- **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "- **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "- **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "- **sex**: Female, Male.\n",
    "- **capital-gain**: continuous.\n",
    "- **capital-loss**: continuous.\n",
    "- **hours-per-week**: continuous.\n",
    "- **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "- **income**: `>50K`, `<=50K`\n",
    "\n",
    "Prediction task is to determine whether a person makes over 50K a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of processing data from last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: \n",
    "* check for and drop meaningless variables\n",
    "* explore our data - are there any variables we should remove?\n",
    "* one-hot encoding of variables\n",
    "* build dummy variables from features we have already\n",
    "* remove hard to interpret variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data \n",
    "df_raw = pd.read_csv(\"adult.csv\", index_col=[0])\n",
    "\n",
    "# Drop meaningless variables\n",
    "df_raw = df_raw.drop(['fnlwgt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of United-States occurrences\n",
    "df_raw = df_raw.drop(['native.country'], axis=1)\n",
    "\n",
    "# One-hot encode categorical variables \n",
    "one_hot_df = pd.get_dummies(df_raw, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the new table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dummy variable for working vs not working ('Without-pay')\n",
    "for col in one_hot_df.columns:\n",
    "    if 'workclass_' in col and ' Without-pay' not in col:\n",
    "        one_hot_df = one_hot_df.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove education.num as it is difficult to interpret (and we have other columns for education levels)\n",
    "# Please complete this line\n",
    "one_hot_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive modelling <a name=\"Modelling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive analysis follows a descriptive stage, once we have:\n",
    " - cleaned the data\n",
    " - created a few derived variables\n",
    " - have a better understanding of the data. \n",
    " \n",
    "Its aim is to build a performing predictive model and to identify the main explanatory variables. This is an iterative process as a variety of modelling approaches are usually tested and new features are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of this section is to accurately predict whether an adult is going to make more than 50K a year and understand the drivers explaining such an income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the proportion of positive and negative labels i.e. the number of rows with income > 50K vs the number of rows where it is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the number of positive and negative rows below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a reasonable amount of data in each class which is good. Sometimes we can have imbalanced classes, where the size of one class is much smaller than the size of the other class. In this case, we may need some additional methods to address this. See https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ for more information if you are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split <a name=\"Train_test_split\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good prediction model must be generalisable - i.e., it should be able to make accurate predictions on new data. Several methods exist, to make our model more generalisable, of which the 'train/test split'.\n",
    "\n",
    "Other methods, such as train/test/validation split and cross-validation take this approach one step further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target variable from the dataset\n",
    "target = one_hot_df['income_>50K']\n",
    "one_hot_df_indep = one_hot_df.drop('income_>50K', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "# a frequent test set size is 20%-30% of the original dataset\n",
    "\n",
    "X = one_hot_df_indep.values\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree <a name=\"Decision_tree\"></a>\n",
    "\n",
    "A decision tree can be perceived as a set of rules which enable to better describe and predict a given phenomenon, e.g., here, a salary `>50K` or `<= 50K`. As its name indicates, a tree is composed of branches (which link the nodes to one another or to the final leaves), nodes (in the middle of the tree) and leaves (higher end of the tree).\n",
    "\n",
    "In our case, the tree splits at each node, on a **rule/condition** (explanatory variable and a set of values - e.g., **age > 38**) according to a criterion (Gini), to better separate `>50K` vs `<= 50K` populations.\n",
    "\n",
    "The decision tree algorithm has several important hyperparameters (cf. below). Understanding these will enable you to avoir overfitting:\n",
    "\n",
    "* the splitting criterion, for each node: Gini (most frequently used), entropy etc.\n",
    "* the maximum depth of the tree: how many branches link the first node to the end leaves?\n",
    "* the minimum sample split: the minimum number of data points in each node, after a split\n",
    "* the minimum samples per leaf: the minimum number of data points in each final leaf\n",
    "* the maximum number of features to consider when looking for the best split *(from the sklearn documentation)*:\n",
    "    * If int, then consider max_features features at each split.\n",
    "    * If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "    * If “auto”, then max_features=sqrt(n_features).\n",
    "    * If “sqrt”, then max_features=sqrt(n_features).\n",
    "    * If “log2”, then max_features=log2(n_features).\n",
    "    * If None, then max_features=n_features.\n",
    "    \n",
    "For a more in depth explanation of how to tune these parameters, please refer to https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "# Initiate your Decision Tree Classifier model\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=20, \n",
    "                             min_samples_leaf=10, max_features=None)\n",
    "\n",
    "# Fit your Decision Tree model to your train model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels on your Test set of independent variables\n",
    "pred_clf = clf.predict(X_test)\n",
    "# Predict probabilities on your Test set of independent variables\n",
    "proba_clf = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We guess the labels correctly 85% of the time! But, can we think of a situation when accuracy is not a good measure of predictive power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also take a look at the confusion matrix, to see where our classifier is making mistakes\n",
    "print(metrics.confusion_matrix(y_test, pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other evaluation metrics...\n",
    "print(metrics.classification_report(y_test, pred_clf))\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, proba_clf[:,1])\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "features_log = pd.DataFrame(clf.feature_importances_, index = one_hot_df_indep.columns.tolist(), \n",
    "                            columns = ['Importance'])\n",
    "features_log = features_log.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "features_log.plot(kind='barh', figsize=(12,9), color = 'blue')\n",
    "plt.xlabel('Feature importance')\n",
    "plt.title('Feature importance for Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced (aka I'm feeling smart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting vs bagging\n",
    "\n",
    "* In this section, we are going to explore new types of models (random forests and XGBoost), which are globally called 'ensemble methods'. \n",
    "\n",
    "* The idea behind ensemble methods is to combine several models to have a better predictive performance. E.g., a single decision tree, is a weak model. If we build several decision trees on samples of the data, the final model will be more robust to outliers and will hence have a better predictive performance. \n",
    "\n",
    "* To understand this concept, we need to have a look at bagging and boosting, which are two sampling methods used in ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bagging** consists in sampling with replacement. Ensemble methods using the bagging approach, build models on each sample of data; the final score of the ensemble model will be a *vote* for classification and an average for regression.\n",
    "\n",
    "* In **Boosting**, data points all have equal weights at the beginning. These weights are increased, if the data points are misclassified (the algorithm will focus on them) and decreased, if they are well classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest <a name=\"Random_forest\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random forest is an example of bagging, and consists of an ensemble of decision trees \n",
    "(built on subsets of the data, which are sampled with replacement). \n",
    "Tree nodes are split on a random subset of features. The amount of features selected is \n",
    "one of the hyperparameters of the model (max_features, here).\n",
    "\n",
    "NB: In general, a smaller subset of variables produces less correlation and as such, a lower error rate.\n",
    "There a few rules of thumb:\n",
    "\n",
    "If M is the number of features in the dataset and m is the max number of features randomly sampled:\n",
    "\n",
    "m << M\n",
    "\n",
    "- for regression: m = M/3\n",
    "- for classification: m = sqrt(M)\n",
    "\n",
    "For a more advanced read on how to tune the hyperparameters of a random forest, please see https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Random Forest Classifier object with the chosen hyperparameters\n",
    "clf_rf = RandomForestClassifier(random_state = 33, n_estimators=50, max_depth=5, min_samples_leaf=30, max_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Random Forest to your train data below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels on your Test set of independent variables\n",
    "pred_rf = \n",
    "# Predict probabilities on your Test set of independent variables\n",
    "proba_rf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the model accuracy below\n",
    "print(\"Accuracy:\", #put your answer here )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take a look again at the confusion matrix, to see where our classifier is making mistakes\n",
    "print(metrics.confusion_matrix(y_test, pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other evaluation metrics...\n",
    "print(metrics.classification_report(y_test, pred_rf))\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, proba_rf[:,1])\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "features_log = pd.DataFrame(clf_rf.feature_importances_, index = one_hot_df_indep.columns.tolist(), \n",
    "                            columns = ['Importance'])\n",
    "features_log = features_log.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "features_log.plot(kind='barh', figsize=(12,9), color = 'blue')\n",
    "plt.xlabel('Feature importance')\n",
    "plt.title('Feature importance for Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some homework..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some additional work, I would first recommend reading through some of the links suggested in this notebook. These will give you a thorough understanding of how to apply these methods in practice, and will be relevant for your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning task: (Advanced)\n",
    "\n",
    "As previously mentioned, it is important to choose 'good' parameters for a random forest, based on the data. One method of doing this is by using cross-validation. In cross-validation, in addition to our usual train and test split of the data we further split our training set into K subsets, called folds. We can then train our Random Forest on K-1 folds and test on the remaining one, repeating to use each fold as the test. We do this for each set of parameters we want to search over, and average the result over the folds. \n",
    "\n",
    "Let's have a go at this....\n",
    "\n",
    "Use https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 as a resource to fill in the gaps yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the parameters of that we used before\n",
    "pprint(clf_rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search over the following parameter settings #TODO: edit this \n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 150, num = 11)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 55, num = 5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [30, 40, 50]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the random grid dictionary with all of the parameters we are searching over: \n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               #complete this here \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model, similarly to how we did before:\n",
    "rf = ...\n",
    "\n",
    "# Declare the random search using 3 fold cross validation, searching over 100 combinations of the parameters \n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit the random search model with our training data below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best choice of hyperparameters found by the random search? Do this below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find out how the accuracy of this model with tuned hyperparameters compares! \n",
    "# Predict the labels on your test set, as we did before, and then print the model accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you improve the accuracy? What a difference the parameters of the model can make!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
